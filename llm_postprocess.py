#!/usr/bin/env python3
"""
LLM-based Post-processing for WhisperX Transcriptions

This script takes an SRT file generated by WhisperX, sends its segments to a 
local LLM (OpenAI API compatible) for verification and correction. If corrected
by LLM, it attempts to re-align with audio using WhisperX alignment. 
Outputs a new, revised SRT file progressively.

Features:
- Connects to a local LLM via OpenAI API standard.
- Uses a sliding window approach to provide context to the LLM.
- Identifies and applies LLM-suggested corrections.
- Attempts to re-align LLM-corrected segments with the original audio.
- Writes output SRT progressively.
"""

import os
import sys
import srt
import time
import torch # Added for whisperx
import whisperx # Added for alignment
from openai import OpenAI
from datetime import timedelta
import numpy as np # Added for audio slicing

# --- FFmpeg Setup (Copied from transcribe_whisperx.py) ---
def setup_ffmpeg():
    """Setup FFmpeg path."""
    # TODO: Consider making ffmpeg_path configurable or automatically detected
    ffmpeg_path = r"C:\Users\vladc\Desktop\shareX\ffmpeg.exe" 
    if os.path.exists(ffmpeg_path):
        ffmpeg_dir = str(os.path.dirname(ffmpeg_path)) # Use os.path.dirname
        current_path = os.environ.get("PATH", "")
        if ffmpeg_dir not in current_path:
            os.environ["PATH"] = f"{ffmpeg_dir};{current_path}"
            print(f"üîß (LLM Script) Added FFmpeg to PATH: {ffmpeg_dir}")
    else:
        print(f"‚ö†Ô∏è (LLM Script) FFmpeg not found at: {ffmpeg_path}")
        print("   Audio loading for non-WAV files might fail.")
        print("   Please ensure FFmpeg is in your system PATH or update the path in setup_ffmpeg().")

# --- Configuration ---
# TODO: User should configure these via command-line arguments or a config file
LLM_API_BASE_URL = "http://localhost:1234/v1"  # Example: For LM Studio
LLM_MODEL_NAME = "mistral-small-3.1-24b-instruct-2503"  # Example: Model name shown in your local LLM server
LLM_API_KEY = "dummy"      # Often "dummy" or not required for local LLMs

# BATCH_SIZE for LLM processing (formerly CONTEXT_WINDOW_SIZE)
# This many segments will be sent to the LLM at once.
LLM_BATCH_SIZE = 5 # Let's start with 5, can be tuned
# CONTEXT_WINDOW_SIZE is no longer used directly for LLM prompt, batching provides context.

DEVICE = "cuda" if torch.cuda.is_available() else "cpu" # Added for whisperx

# --- Helper Function to format srt block ---
def format_srt_segment(index: int, start_time: timedelta, end_time: timedelta, text: str) -> str:
    """Formats a single segment into an SRT block string."""
    def format_timedelta(td):
        total_seconds = td.total_seconds()
        hours = int(total_seconds // 3600)
        minutes = int((total_seconds % 3600) // 60)
        seconds = total_seconds % 60
        return f"{hours:02d}:{minutes:02d}:{seconds:06.3f}".replace('.', ',')

    return f"{index}\n{format_timedelta(start_time)} --> {format_timedelta(end_time)}\n{text}\n\n"

def load_srt_file(srt_path: str) -> list:
    """Loads an SRT file and returns a list of subtitle objects."""
    print(f"üîÑ Loading SRT file: {srt_path}")
    try:
        with open(srt_path, 'r', encoding='utf-8') as f:
            subs = list(srt.parse(f.read()))
        print(f"‚úÖ SRT file loaded: {len(subs)} segments")
        return subs
    except Exception as e:
        print(f"‚ùå Error loading SRT file {srt_path}: {e}")
        sys.exit(1)

def get_contextual_text(segments: list, current_index: int, window_size: int) -> tuple[str, str, str]:
    """
    Extracts text from the current segment and its surrounding context window.
    Returns (context_before, current_text, context_after).
    """
    current_segment = segments[current_index]
    current_text = current_segment.content

    start_idx = max(0, current_index - window_size)
    end_idx = min(len(segments), current_index + window_size + 1)

    context_before_segments = segments[start_idx:current_index]
    context_after_segments = segments[current_index + 1:end_idx]

    context_before = "\n".join([seg.content for seg in context_before_segments])
    context_after = "\n".join([seg.content for seg in context_after_segments])
    
    return context_before, current_text, context_after

def query_local_llm_batch(client: OpenAI, batched_prompt: str, model_name: str, num_segments_in_batch: int) -> list[str | None] | None:
    """
    Sends a batched prompt to the local LLM and returns a list of corrected text strings,
    one for each segment in the batch.
    Expects LLM to respond with:
    SEGMENT_1: <corrected text for segment 1>
    SEGMENT_2: <corrected text for segment 2>
    ...
    """
    try:
        # print(f"üí¨ Sending BATCHED prompt to LLM ({model_name}) for {num_segments_in_batch} segments:\n---\n{batched_prompt[:700]}...\n---")
        completion = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that meticulously reviews and corrects transcript segments. Your primary goal is to improve accuracy and readability, focusing on correcting transcription errors (especially misheard words that sound similar but make more sense in context) while preserving original meaning and speaker intent. Use diacritics. Make minimal changes. If a segment is already good, return it as is. Ensure your output strictly follows the specified format for each segment."},
                {"role": "user", "content": batched_prompt}
            ],
            temperature=0.2,
        )
        raw_response = completion.choices[0].message.content.strip()
        # print(f"üó£Ô∏è LLM BATCHED raw response:\n---\n{raw_response[:700]}...\n---")

        corrected_texts = [None] * num_segments_in_batch
        found_any = False
        for line in raw_response.split('\n'):
            line_upper = line.upper()
            if line_upper.startswith("SEGMENT_"):
                try:
                    parts = line.split(":", 1)
                    segment_num_str = parts[0].replace("SEGMENT_", "").strip()
                    segment_idx = int(segment_num_str) - 1 # 0-indexed

                    if 0 <= segment_idx < num_segments_in_batch:
                        corrected_texts[segment_idx] = parts[1].strip() if len(parts) > 1 else ""
                        found_any = True
                    else:
                        print(f"‚ö†Ô∏è LLM response parsing: Segment index {segment_idx+1} out of range for batch size {num_segments_in_batch}.")
                except ValueError:
                    print(f"‚ö†Ô∏è LLM response parsing: Could not parse segment number from line: {line}")
                except IndexError:
                    print(f"‚ö†Ô∏è LLM response parsing: Line format error (missing ':') in line: {line}")
        
        if not found_any:
            print(f"‚ö†Ô∏è LLM BATCHED response parsing: No 'SEGMENT_X:' lines found. LLM might not have followed format.")
            print(f"   LLM Raw Response: {raw_response}")
            # If no segments are parsed, it's safer to assume failure for the whole batch
            return None 
            
        # print(f"üó£Ô∏è LLM BATCHED parsed corrected texts: {corrected_texts}")
        return corrected_texts

    except Exception as e:
        print(f"‚ö†Ô∏è Error querying LLM (batch): {e}")
        return None

def build_llm_batch_prompt(segments_in_batch: list[srt.Subtitle], segment_texts_for_llm: list[str]) -> str:
    """Builds the prompt for the LLM for a batch of segments."""
    
    prompt_header = (
        f"You will be provided with a batch of {len(segments_in_batch)} numbered transcript segments.\n"
        "Please review and correct EACH segment individually based on the overall context of the batch. \n"
        "Focus on fixing potential transcription errors, especially words that might have been misunderstood by the VTT model (think of words that SOUND similar but make more sense in context). \n"
        "Preserve the original meaning and use diacritics. \n"
        "Corrected segments will be used for audio alignment, so accuracy of individual words is important. \n"
        "If a segment is already accurate, return it unchanged.\n\n"
        "For each segment, return its corrected text in the following format, ensuring each segment response is on a new line:\n"
        "SEGMENT_1: <corrected text for segment 1>\n"
        "SEGMENT_2: <corrected text for segment 2>\n"
        f"SEGMENT_{len(segments_in_batch)}: <corrected text for segment N>\n\n"
        "Do not include any other explanations or text outside this format. Return ONLY the 'SEGMENT_X: <text>' lines.\n\n"
        "BATCH OF SEGMENTS TO CORRECT:\n"
    )
    
    prompt_segments = []
    for i, text_for_llm in enumerate(segment_texts_for_llm):
        prompt_segments.append(f"SEGMENT_{i+1}: {text_for_llm}")
        
    full_prompt = prompt_header + "\n".join(prompt_segments)
    return full_prompt

def write_revised_srt(segments: list, output_path: str) -> None:
    """Writes the list of srt.Subtitle objects to an SRT file."""
    print(f"üíæ Writing revised SRT file to: {output_path}")
    try:
        composed_srt = srt.compose(segments)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(composed_srt)
        print(f"‚úÖ Revised SRT file saved: {output_path}")
    except Exception as e:
        print(f"‚ùå Error writing revised SRT file {output_path}: {e}")

# Updated realign_segment_with_audio function
def realign_segment_with_audio(
    original_audio_data: np.ndarray, 
    audio_sample_rate: int,
    alignment_model, # Simpler type hint, was: whisperx.asr.WhisperXAIModel,
    alignment_metadata: dict,
    text_to_align: str, 
    segment_start_time: float, # in seconds
    segment_end_time: float,   # in seconds
    language_code: str
) -> str | None:
    """
    Re-aligns the given text with the specified audio chunk.
    Returns the aligned text if successful, otherwise None.
    """
    # print(f"üîä Attempting to re-align: \"{text_to_align[:50]}...\"")
    try:
        # 1. Extract the audio chunk
        start_sample = int(segment_start_time * audio_sample_rate)
        end_sample = int(segment_end_time * audio_sample_rate)
        audio_chunk = original_audio_data[start_sample:end_sample]

        if audio_chunk.size == 0:
            print("   ‚ö†Ô∏è Audio chunk is empty. Cannot align.")
            return None

        # 2. Create a dummy segment structure for whisperx.align
        # whisperx.align expects a list of segments.
        # We need to ensure the text has some substance for alignment.
        if not text_to_align or len(text_to_align.split()) < 1:
             print("   ‚ö†Ô∏è Text to align is too short or empty. Cannot align.")
             return None

        dummy_segment = {
            'text': text_to_align,
            'start': 0.0, # Timestamps are relative to the chunk now
            'end': audio_chunk.shape[0] / audio_sample_rate,
            'words': [] # whisperx.align will populate this if char_alignments=True
        }
        
        # 3. Perform alignment
        # Note: The `align` function can modify segments in-place or return new ones.
        # We are interested if it can align this text to this audio chunk.
        # The result will contain word-level timestamps *relative to the chunk*.
        aligned_result = whisperx.align(
            [dummy_segment], # Corrected: segments is the first positional argument
            model=alignment_model,
            align_model_metadata=alignment_metadata,
            audio=audio_chunk, # Pass the chunk
            device=DEVICE,
            return_char_alignments=False # Word-level is enough for validation
        )

        if aligned_result and aligned_result['segments'] and aligned_result['segments'][0].get('text'):
            # print(f"   ‚úÖ Re-alignment successful for: \"{aligned_result['segments'][0]['text'][:50]}...\"")
            # For now, we just confirm it aligned. The actual text might be slightly
            # refined by the aligner (e.g. casing), so we use the aligned text.
            return aligned_result['segments'][0]['text']
        else:
            print(f"   ‚ö†Ô∏è Re-alignment failed to produce a result for: \"{text_to_align[:50]}...\"")
            return None

    except Exception as e:
        print(f"   ‚ùå Error during re-alignment: {e}")
        # import traceback
        # traceback.print_exc()
        return None

def main():
    if len(sys.argv) < 3:
        print("Usage: python llm_postprocess.py <input_srt_file> <original_audio_file> [output_srt_file]")
        print("Example: python llm_postprocess.py whisperx_output.srt audio.wav llm_revised_output.srt")
        sys.exit(1)

    setup_ffmpeg() # Call FFmpeg setup

    input_srt_path = sys.argv[1]
    original_audio_path = sys.argv[2] 
    
    if not os.path.exists(input_srt_path):
        print(f"‚ùå Input SRT file not found: {input_srt_path}")
        sys.exit(1)
    if not os.path.exists(original_audio_path):
        print(f"‚ùå Original audio file not found: {original_audio_path}")
        sys.exit(1) 

    output_srt_path = ""
    if len(sys.argv) > 3:
        output_srt_path = sys.argv[3]
    else:
        base, ext = os.path.splitext(input_srt_path)
        output_srt_path = f"{base}_llm_batched_aligned{ext}"

    print("üöÄ Starting LLM Post-processing with BATCHING and Re-alignment...")
    print(f"   Input SRT: {input_srt_path}")
    print(f"   Audio File: {original_audio_path}")
    print(f"   Output SRT: {output_srt_path}")
    print(f"   LLM Endpoint: {LLM_API_BASE_URL}")
    print(f"   LLM Model: {LLM_MODEL_NAME}")
    print(f"   LLM Batch Size: {LLM_BATCH_SIZE}") # Updated print
    print(f"   Device for alignment: {DEVICE}")

    # Load original audio (once)
    print(f"üéµ Loading original audio file: {original_audio_path}...")
    try:
        # Assuming whisperx.load_audio returns a NumPy array and sample rate
        # The actual sample rate of whisperx.load_audio is 16000 Hz
        audio_data_full = whisperx.load_audio(original_audio_path)
        AUDIO_SAMPLE_RATE = 16000 # WhisperX standard
        print(f"‚úÖ Original audio loaded successfully. Shape: {audio_data_full.shape}, Sample Rate: {AUDIO_SAMPLE_RATE} Hz")
    except Exception as e:
        print(f"‚ùå Failed to load original audio: {e}")
        sys.exit(1)

    # Load alignment model (once)
    # We need language for this. Let's try to get it from the first segment or assume 'en'
    # This is a simplification. A robust way would be to pass lang or detect from whisperx output.
    # For now, we'll try to infer it or default.
    original_segments = load_srt_file(input_srt_path)
    if not original_segments:
        print("‚ùå No segments found in input SRT. Exiting.")
        sys.exit(1)

    # Attempt to infer language from the first segment if WhisperX added it
    # This part is a guess; WhisperX SRTs don't usually store language.
    # We'll need a reliable way to get the language code, e.g. from transcribe_whisperx.py's output or as a param.
    # For now, let's make it a TODO or default.
    # language_code = "en" # Defaulting
    # print(f"‚ö†Ô∏è Assuming language code '{language_code}' for alignment. This should be made more robust.")
    
    # --- Determine Language for Alignment ---
    # The best way is to get it from the main transcription process.
    # If your transcribe_whisperx.py can output a small info file with the detected language,
    # that would be ideal. Otherwise, you might need to pass it as a command-line argument.
    # For now, as a placeholder, let's try to take it from an environment variable or default.
    LANGUAGE_CODE_FOR_ALIGNMENT = os.getenv("TRANSCRIPTION_LANGUAGE", "ro") # Default to 'ro' or your common lang
    print(f"üó£Ô∏è Using language code for alignment: {LANGUAGE_CODE_FOR_ALIGNMENT} (set via TRANSCRIPTION_LANGUAGE env var or default)")


    print(f"Loading alignment model for language: {LANGUAGE_CODE_FOR_ALIGNMENT}...")
    try:
        align_model, align_metadata = whisperx.load_align_model(
            language_code=LANGUAGE_CODE_FOR_ALIGNMENT, device=DEVICE
        )
        print("‚úÖ Alignment model loaded successfully.")
    except Exception as e:
        print(f"‚ùå Failed to load alignment model: {e}")
        print("   Make sure you have the correct language code and model files available.")
        sys.exit(1)
    
    revised_segments_count = 0
    llm_corrections_suggested_count = 0 # Renamed for clarity
    alignments_attempted = 0
    alignments_successful = 0

    try:
        client = OpenAI(base_url=LLM_API_BASE_URL, api_key=LLM_API_KEY)
    except Exception as e:
        print(f"‚ùå Failed to initialize OpenAI client for local LLM: {e}")
        sys.exit(1)

    total_original_segments = len(original_segments)

    with open(output_srt_path, 'w', encoding='utf-8') as outfile:
        print(f"üìù Output SRT will be written progressively to: {output_srt_path}")
        
        for i in range(0, total_original_segments, LLM_BATCH_SIZE):
            batch_original_segments = original_segments[i : i + LLM_BATCH_SIZE]
            if not batch_original_segments:
                continue

            print(f"Processing batch starting with segment {batch_original_segments[0].index} (Batch {i//LLM_BATCH_SIZE + 1}/{(total_original_segments + LLM_BATCH_SIZE -1)//LLM_BATCH_SIZE})...")

            batch_texts_for_llm = []
            batch_speaker_prefixes = []
            
            for seg_in_batch in batch_original_segments:
                original_content = seg_in_batch.content
                speaker_prefix = ""
                text_for_llm = original_content
                if original_content.startswith("[") and "]" in original_content:
                    parts = original_content.split("]", 1)
                    if len(parts) > 1:
                        speaker_tag = parts[0] + "]"
                        text_part = parts[1].strip()
                        if "SPEAKER" in speaker_tag.upper():
                            speaker_prefix = speaker_tag + " "
                            text_for_llm = text_part
                batch_texts_for_llm.append(text_for_llm)
                batch_speaker_prefixes.append(speaker_prefix)

            batched_llm_prompt = build_llm_batch_prompt(batch_original_segments, batch_texts_for_llm)
            llm_batch_corrected_texts = query_local_llm_batch(client, batched_llm_prompt, LLM_MODEL_NAME, len(batch_original_segments))

            if llm_batch_corrected_texts is None:
                print(f"  ‚ö†Ô∏è LLM batch query failed. Reverting to original text for this batch.")
                # Write original segments for this batch
                for k, original_segment_in_batch in enumerate(batch_original_segments):
                    revised_segments_count += 1
                    srt_block = format_srt_segment(
                        revised_segments_count,
                        original_segment_in_batch.start,
                        original_segment_in_batch.end,
                        original_segment_in_batch.content # Full original content
                    )
                    outfile.write(srt_block)
                    outfile.flush()
                time.sleep(0.1) # Small delay even on failure
                continue # Move to the next batch

            # Process each segment within the batch based on LLM response
            for k, original_segment_in_batch in enumerate(batch_original_segments):
                current_text_for_llm = batch_texts_for_llm[k]
                speaker_prefix = batch_speaker_prefixes[k]
                llm_suggested_text_for_this_segment = llm_batch_corrected_texts[k]

                final_text_for_segment = current_text_for_llm # Default to original (text part only)
                llm_correction_applied_this_segment = False

                if llm_suggested_text_for_this_segment is not None and \
                   llm_suggested_text_for_this_segment.strip().lower() != current_text_for_llm.strip().lower():
                    
                    print(f"  ‚ú® LLM suggests change for segment {original_segment_in_batch.index} (in batch):")
                    print(f"     Original: \"{current_text_for_llm[:100]}...\"")
                    print(f"     LLM Says: \"{llm_suggested_text_for_this_segment[:100]}...\"")
                    llm_corrections_suggested_count += 1
                    alignments_attempted += 1

                    segment_start_seconds = original_segment_in_batch.start.total_seconds()
                    segment_end_seconds = original_segment_in_batch.end.total_seconds()
                    
                    aligned_text = realign_segment_with_audio(
                        audio_data_full, AUDIO_SAMPLE_RATE,
                        align_model, align_metadata,
                        llm_suggested_text_for_this_segment,
                        segment_start_seconds, segment_end_seconds,
                        LANGUAGE_CODE_FOR_ALIGNMENT
                    )

                    if aligned_text:
                        print(f"    ‚úÖ Alignment successful. Using LLM's suggestion: \"{aligned_text[:100]}...\"")
                        final_text_for_segment = aligned_text
                        llm_correction_applied_this_segment = True
                        alignments_successful +=1
                    else:
                        print(f"    ‚ö†Ô∏è Alignment failed for LLM's suggestion. Reverting to original text for this segment.")
                
                full_segment_content_to_write = speaker_prefix + final_text_for_segment
                revised_segments_count += 1
                srt_block = format_srt_segment(
                    revised_segments_count,
                    original_segment_in_batch.start,
                    original_segment_in_batch.end,
                    full_segment_content_to_write
                )
                outfile.write(srt_block)
                outfile.flush()
                if llm_correction_applied_this_segment:
                    print(f"    üìù Segment {original_segment_in_batch.index} written to output with LLM correction.")
            
            time.sleep(0.1) # Delay per batch

    print("\nüèÅ LLM Post-processing complete.")
    print(f"   Total original segments: {total_original_segments}")
    print(f"   Segments written to new file: {revised_segments_count}")
    print(f"   LLM corrections suggested: {llm_corrections_suggested_count}")
    print(f"   Alignments attempted: {alignments_attempted}")
    print(f"   Alignments successful (LLM text used): {alignments_successful}")
    print(f"   Revised SRT saved to: {output_srt_path}")

if __name__ == "__main__":
    main() 